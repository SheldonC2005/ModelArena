{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08361897",
   "metadata": {},
   "source": [
    "# üéØ DEEPFAKE DETECTION - INFERENCE PIPELINE\n",
    "\n",
    "**Mission:** Generate predictions on test set using trained InceptionResNetV2 + Attention model\n",
    "\n",
    "**Model Architecture:**\n",
    "- CNN: InceptionResNetV2 (feature extractor)\n",
    "- Temporal: Attention Pooling (learns frame importance)\n",
    "- Single model trained on 70:30 split\n",
    "\n",
    "**Inference Strategy:**\n",
    "- Test-Time Augmentation (TTA): 5 variants per video\n",
    "- Batch size: 1 (optimized for 4GB VRAM)\n",
    "- Mixed precision: FP16\n",
    "- Frame count testing: Compare 24 vs 32 vs 48 frames\n",
    "\n",
    "**Output:** `PREDICTIONS.CSV` with columns:\n",
    "- `filename` - Video filename\n",
    "- `label` - Predicted class (0=Real, 1=Fake)\n",
    "- `probability` - Confidence score of predicted class (0.0 to 1.0)\n",
    "\n",
    "**Hardware:** RTX 3050 Laptop (4GB VRAM)  \n",
    "**Estimated Time:** \n",
    "- Frame testing (10 videos): ~2 minutes\n",
    "- Full inference (200 videos √ó 5 TTA): ~25-35 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üìã INFERENCE CHECKLIST\n",
    "- [ ] GPU verified\n",
    "- [ ] Model checkpoint loaded\n",
    "- [ ] Frame count tested (24/32/48)\n",
    "- [ ] Configuration set based on test results\n",
    "- [ ] Full inference complete\n",
    "- [ ] predictions.csv generated and validated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf215f",
   "metadata": {},
   "source": [
    "## üîß STEP 1: Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and CUDA availability\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(f\"System: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"\\n‚úÖ GPU ready for inference!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: GPU not detected! Inference will be slow on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd316fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Model library\n",
    "import timm\n",
    "\n",
    "# GPU monitoring\n",
    "import pynvml\n",
    "try:\n",
    "    pynvml.nvmlInit()\n",
    "    gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    print(\"‚úÖ NVML initialized for GPU monitoring\")\n",
    "except:\n",
    "    gpu_handle = None\n",
    "    print(\"‚ö†Ô∏è NVML not available - VRAM monitoring limited\")\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n‚úÖ All libraries imported!\")\n",
    "print(f\"Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3827b9",
   "metadata": {},
   "source": [
    "## üíæ STEP 2: Setup Paths & Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfa019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup local paths (Windows)\n",
    "BASE_DIR = Path(r\"D:\\Data\\Github\\SheldonC2005\\ModelArena\")\n",
    "BASE_PATH = BASE_DIR / \"archive\"\n",
    "TEST_PATH = BASE_PATH / \"test\"\n",
    "TEST_CSV_PATH = BASE_PATH / \"test_public.csv\"\n",
    "\n",
    "# Model and output paths\n",
    "MODELS_PATH = BASE_DIR / \"models\"\n",
    "MODEL_CHECKPOINT = MODELS_PATH / \"inception_resnet_v2_best.pt\"\n",
    "OUTPUT_DIR = BASE_DIR / \"SUBMISSION_DIRECTORY\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "OUTPUT_CSV = OUTPUT_DIR / \"PREDICTIONS.CSV\"\n",
    "\n",
    "print(f\"‚úÖ Paths configured:\")\n",
    "print(f\"   Base directory: {BASE_DIR}\")\n",
    "print(f\"   Test videos: {TEST_PATH}\")\n",
    "print(f\"   Model checkpoint: {MODEL_CHECKPOINT}\")\n",
    "print(f\"   Output CSV: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f715138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test CSV and verify all videos exist\n",
    "print(\"üìä Loading test data and verifying videos...\\n\")\n",
    "\n",
    "# Check if model checkpoint exists\n",
    "if not MODEL_CHECKPOINT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Model checkpoint not found: {MODEL_CHECKPOINT}\\n\"\n",
    "        f\"   Please train the model first using TRAINING_PIPELINE.ipynb\"\n",
    "    )\n",
    "print(f\"‚úÖ Model checkpoint found ({MODEL_CHECKPOINT.stat().st_size / 1024**2:.2f} MB)\")\n",
    "\n",
    "# Load test CSV\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "print(f\"‚úÖ Test CSV loaded: {len(test_df)} samples\")\n",
    "print(f\"   Columns: {list(test_df.columns)}\")\n",
    "\n",
    "# Add full video paths\n",
    "test_df['video_path'] = test_df['filename'].apply(lambda x: str(TEST_PATH / x))\n",
    "\n",
    "# FULL VERIFICATION: Check every video exists\n",
    "print(f\"\\nüîç Verifying all {len(test_df)} test videos exist...\")\n",
    "missing_videos = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    if not Path(row['video_path']).exists():\n",
    "        missing_videos.append(row['filename'])\n",
    "\n",
    "if missing_videos:\n",
    "    print(f\"‚ùå ERROR: {len(missing_videos)} videos not found!\")\n",
    "    print(f\"   First few missing: {missing_videos[:10]}\")\n",
    "    raise FileNotFoundError(f\"Missing {len(missing_videos)} test videos\")\n",
    "else:\n",
    "    print(f\"‚úÖ All {len(test_df)} test videos verified!\")\n",
    "\n",
    "# Check if output CSV already exists\n",
    "if OUTPUT_CSV.exists():\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {OUTPUT_CSV.name} already exists and will be overwritten!\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Ready for inference on {len(test_df)} test videos\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d0259",
   "metadata": {},
   "source": [
    "## üèóÔ∏è STEP 3: Model Architecture & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d1322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference configuration\n",
    "class InferenceConfig:\n",
    "    # Model settings (MUST match training config)\n",
    "    IMG_SIZE = 299  # InceptionResNetV2 input size\n",
    "    FEATURE_DIM = 1536  # InceptionResNetV2 feature dimension\n",
    "    \n",
    "    # Inference settings\n",
    "    BATCH_SIZE = 1  # Ultra-safe for 4GB VRAM\n",
    "    NUM_WORKERS = 0  # Safe for Windows\n",
    "    MIXED_PRECISION = True  # FP16 for efficiency\n",
    "    \n",
    "    # VRAM safety thresholds\n",
    "    VRAM_EMERGENCY_THRESHOLD = 0.90  # Clear cache if VRAM > 90%\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "print(\"‚úÖ Inference configuration loaded!\")\n",
    "print(f\"   Image size: {config.IMG_SIZE}√ó{config.IMG_SIZE}\")\n",
    "print(f\"   Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"   Mixed precision: {config.MIXED_PRECISION}\")\n",
    "print(f\"   VRAM emergency threshold: {config.VRAM_EMERGENCY_THRESHOLD*100:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20683a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN + Attention Model Architecture (MUST match training exactly)\n",
    "class CNN_Attention_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    InceptionResNetV2 (Feature Extractor) + Attention Pooling (Temporal Modeling)\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=1536, attention_dim=256, fc_dropout=0.5, num_classes=2):\n",
    "        super(CNN_Attention_Model, self).__init__()\n",
    "        \n",
    "        # Load pretrained InceptionResNetV2\n",
    "        self.cnn = timm.create_model('inception_resnet_v2', pretrained=True, num_classes=0)\n",
    "        \n",
    "        # Attention mechanism for temporal modeling\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, attention_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fc_dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_frames, C, H, W]\n",
    "        batch_size, num_frames, C, H, W = x.shape\n",
    "        \n",
    "        # Reshape to process all frames: [batch_size * num_frames, C, H, W]\n",
    "        x = x.view(batch_size * num_frames, C, H, W)\n",
    "        \n",
    "        # Extract features from CNN\n",
    "        features = self.cnn(x)  # [batch_size * num_frames, feature_dim]\n",
    "        \n",
    "        # Reshape back to sequence: [batch_size, num_frames, feature_dim]\n",
    "        features = features.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attn_scores = self.attention(features)  # [batch_size, num_frames, 1]\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)  # [batch_size, num_frames, 1]\n",
    "        \n",
    "        # Weighted sum of features\n",
    "        context = torch.sum(features * attn_weights, dim=1)  # [batch_size, feature_dim]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.fc(context)  # [batch_size, num_classes]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ CNN_Attention_Model architecture defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed13d49",
   "metadata": {},
   "source": [
    "## üì¶ STEP 4: Load Trained Model with Thorough Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0971754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint with thorough verification\n",
    "print(f\"üì¶ Loading model from: {MODEL_CHECKPOINT}\\n\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(MODEL_CHECKPOINT, map_location=DEVICE)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHECKPOINT VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify checkpoint structure\n",
    "required_keys = ['model_state_dict', 'config', 'val_acc', 'val_f1', 'val_auc']\n",
    "missing_keys = [key for key in required_keys if key not in checkpoint]\n",
    "if missing_keys:\n",
    "    raise KeyError(f\"‚ùå Checkpoint missing required keys: {missing_keys}\")\n",
    "print(\"‚úÖ Checkpoint structure valid\")\n",
    "\n",
    "# Verify config\n",
    "required_config = ['attention_dim', 'fc_dropout', 'feature_dim']\n",
    "missing_config = [key for key in required_config if key not in checkpoint['config']]\n",
    "if missing_config:\n",
    "    raise KeyError(f\"‚ùå Checkpoint config missing: {missing_config}\")\n",
    "print(\"‚úÖ Checkpoint config valid\")\n",
    "\n",
    "# Verify architecture compatibility\n",
    "if checkpoint['config'].get('feature_dim') != config.FEATURE_DIM:\n",
    "    raise ValueError(\n",
    "        f\"‚ùå Architecture mismatch! \"\n",
    "        f\"Checkpoint feature_dim={checkpoint['config'].get('feature_dim')}, \"\n",
    "        f\"Expected {config.FEATURE_DIM}\"\n",
    "    )\n",
    "print(\"‚úÖ Architecture compatibility verified\")\n",
    "\n",
    "# Display checkpoint information\n",
    "print(f\"\\nCheckpoint Training Results:\")\n",
    "print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "print(f\"  Validation Accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "print(f\"  Validation F1: {checkpoint['val_f1']:.4f}\")\n",
    "print(f\"  Validation AUC: {checkpoint['val_auc']:.4f}\")\n",
    "\n",
    "# Get model hyperparameters from checkpoint\n",
    "attention_dim = checkpoint['config']['attention_dim']\n",
    "fc_dropout = checkpoint['config']['fc_dropout']\n",
    "\n",
    "print(f\"\\nModel Hyperparameters:\")\n",
    "print(f\"  Attention dimension: {attention_dim}\")\n",
    "print(f\"  FC dropout: {fc_dropout}\")\n",
    "\n",
    "# Create model with checkpoint hyperparameters\n",
    "model = CNN_Attention_Model(\n",
    "    feature_dim=config.FEATURE_DIM,\n",
    "    attention_dim=attention_dim,\n",
    "    fc_dropout=fc_dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load trained weights\n",
    "try:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"\\n‚úÖ Model weights loaded successfully!\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"‚ùå Failed to load model weights: {e}\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "print(\"‚úÖ Model set to evaluation mode\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ MODEL READY FOR INFERENCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Clear memory\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame testing function\n",
    "def test_frame_counts(model, test_videos, frame_counts=[24, 32, 48], device=DEVICE):\n",
    "    \"\"\"\n",
    "    Test different frame counts on a subset of videos\n",
    "    Returns: Dictionary with results for each frame count\n",
    "    \"\"\"\n",
    "    # Simple augmentation (no TTA for testing, just resize + normalize)\n",
    "    transform = A.Compose([\n",
    "        A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Inception normalization\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    results = {fc: {'predictions': [], 'confidences': []} for fc in frame_counts}\n",
    "    \n",
    "    print(f\"üß™ Testing {len(frame_counts)} frame counts on {len(test_videos)} videos...\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, row in test_videos.iterrows():\n",
    "            video_path = row['video_path']\n",
    "            filename = row['filename']\n",
    "            \n",
    "            print(f\"Processing {filename}...\")\n",
    "            \n",
    "            for num_frames in frame_counts:\n",
    "                # Extract frames\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                \n",
    "                if total_frames <= 0:\n",
    "                    cap.release()\n",
    "                    # Use default prediction for corrupted video\n",
    "                    results[num_frames]['predictions'].append(0)\n",
    "                    results[num_frames]['confidences'].append(0.5)\n",
    "                    continue\n",
    "                \n",
    "                # Uniformly sample frames\n",
    "                indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "                frames = []\n",
    "                \n",
    "                for idx_frame in indices:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx_frame)\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        frames.append(frame)\n",
    "                    else:\n",
    "                        if len(frames) > 0:\n",
    "                            frames.append(frames[-1])\n",
    "                        else:\n",
    "                            frames.append(np.zeros((config.IMG_SIZE, config.IMG_SIZE, 3), dtype=np.uint8))\n",
    "                \n",
    "                cap.release()\n",
    "                \n",
    "                # Transform frames\n",
    "                transformed_frames = []\n",
    "                for frame in frames:\n",
    "                    augmented = transform(image=frame)\n",
    "                    transformed_frames.append(augmented['image'])\n",
    "                \n",
    "                frames_tensor = torch.stack(transformed_frames).unsqueeze(0).to(device)  # [1, num_frames, C, H, W]\n",
    "                \n",
    "                # Forward pass\n",
    "                with autocast(enabled=config.MIXED_PRECISION):\n",
    "                    output = model(frames_tensor)\n",
    "                    probs = F.softmax(output, dim=1)[0]  # [2]\n",
    "                \n",
    "                pred_label = torch.argmax(probs).item()\n",
    "                confidence = torch.max(probs).item()\n",
    "                \n",
    "                results[num_frames]['predictions'].append(pred_label)\n",
    "                results[num_frames]['confidences'].append(confidence)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Frame testing function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11e4027",
   "metadata": {},
   "source": [
    "## üß™ STEP 5: Frame Count Testing (24 vs 32 vs 48 frames)\n",
    "\n",
    "**Purpose:** Test different frame counts to find optimal accuracy  \n",
    "**Method:** Run inference on first 10 test videos with 24, 32, and 48 frames  \n",
    "**Recommendation:** Choose frame count with highest average confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run frame count testing on first 10 videos\n",
    "test_videos = test_df.head(10)\n",
    "frame_test_results = test_frame_counts(model, test_videos, frame_counts=[24, 32, 48])\n",
    "\n",
    "# Display detailed results\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"FRAME COUNT COMPARISON RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Calculate statistics\n",
    "for num_frames in [24, 32, 48]:\n",
    "    avg_conf = np.mean(frame_test_results[num_frames]['confidences'])\n",
    "    predictions = frame_test_results[num_frames]['predictions']\n",
    "    print(f\"\\n{num_frames} frames:\")\n",
    "    print(f\"  Average confidence: {avg_conf:.4f}\")\n",
    "    print(f\"  Predicted Real (0): {predictions.count(0)}\")\n",
    "    print(f\"  Predicted Fake (1): {predictions.count(1)}\")\n",
    "\n",
    "# Find best frame count (highest average confidence)\n",
    "best_frame_count = max([24, 32, 48], \n",
    "                       key=lambda fc: np.mean(frame_test_results[fc]['confidences']))\n",
    "best_confidence = np.mean(frame_test_results[best_frame_count]['confidences'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(f\"üìä RECOMMENDATION: Use {best_frame_count} frames (highest avg confidence: {best_confidence:.4f})\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Display per-video comparison\n",
    "print(\"\\nüìã Per-Video Comparison:\")\n",
    "print(f\"{'Video':<20} | {'24 frames':<20} | {'32 frames':<20} | {'48 frames':<20}\")\n",
    "print(\"-\" * 90)\n",
    "for idx in range(len(test_videos)):\n",
    "    video_name = test_videos.iloc[idx]['filename'][:18]\n",
    "    results_24 = f\"L={frame_test_results[24]['predictions'][idx]}, C={frame_test_results[24]['confidences'][idx]:.3f}\"\n",
    "    results_32 = f\"L={frame_test_results[32]['predictions'][idx]}, C={frame_test_results[32]['confidences'][idx]:.3f}\"\n",
    "    results_48 = f\"L={frame_test_results[48]['predictions'][idx]}, C={frame_test_results[48]['confidences'][idx]:.3f}\"\n",
    "    print(f\"{video_name:<20} | {results_24:<20} | {results_32:<20} | {results_48:<20}\")\n",
    "\n",
    "print(\"\\n‚úÖ Frame testing complete! Review results above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec9709",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è STEP 6: Set Configuration Based on Test Results\n",
    "\n",
    "**Instructions:** Review the frame count testing results above and set FRAMES_PER_VIDEO below.  \n",
    "The recommended value (highest avg confidence) is auto-filled, but you can change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f416ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set frames per video based on test results\n",
    "# Recommended value is auto-set, but you can change to 24, 32, or 48\n",
    "\n",
    "FRAMES_PER_VIDEO = best_frame_count  # Auto-recommended based on test results\n",
    "\n",
    "print(f\"‚úÖ Configuration set:\")\n",
    "print(f\"   FRAMES_PER_VIDEO = {FRAMES_PER_VIDEO}\")\n",
    "print(f\"   (Based on test results: highest avg confidence)\")\n",
    "print(f\"\\n‚ö†Ô∏è If you want to use a different value, change FRAMES_PER_VIDEO above and re-run this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6aeaf1",
   "metadata": {},
   "source": [
    "## üîÑ STEP 7: Test-Time Augmentation (TTA) & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d58a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5 TTA variants\n",
    "def get_tta_transforms():\n",
    "    \"\"\"\n",
    "    Returns 5 augmentation variants for Test-Time Augmentation\n",
    "    All use Inception normalization (mean=0.5, std=0.5)\n",
    "    \"\"\"\n",
    "    tta_transforms = [\n",
    "        # Variant 1: Original (resize + normalize only)\n",
    "        A.Compose([\n",
    "            A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        \n",
    "        # Variant 2: Horizontal flip\n",
    "        A.Compose([\n",
    "            A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "            A.HorizontalFlip(p=1.0),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        \n",
    "        # Variant 3: Shift and scale (mild version of training aug)\n",
    "        A.Compose([\n",
    "            A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=0, p=1.0),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        \n",
    "        # Variant 4: Brightness increase\n",
    "        A.Compose([\n",
    "            A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0, p=1.0),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        \n",
    "        # Variant 5: Brightness decrease\n",
    "        A.Compose([\n",
    "            A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "            A.RandomBrightnessContrast(brightness_limit=-0.1, contrast_limit=0, p=1.0),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    return tta_transforms\n",
    "\n",
    "print(\"‚úÖ TTA transforms defined (5 variants)!\")\n",
    "print(\"   1. Original\")\n",
    "print(\"   2. Horizontal flip\")\n",
    "print(\"   3. Shift + Scale\")\n",
    "print(\"   4. Brightness increase (+0.1)\")\n",
    "print(\"   5. Brightness decrease (-0.1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Dataset for inference (optimized, no training-specific code)\n",
    "class VideoInferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for video inference with TTA support\n",
    "    Extracts frames on-the-fly and applies specified augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, num_frames, transform):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.img_size = config.IMG_SIZE\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def extract_frames(self, video_path):\n",
    "        \"\"\"Extract uniformly spaced frames from video\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Handle corrupted or empty videos\n",
    "        if total_frames <= 0:\n",
    "            cap.release()\n",
    "            return [np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8) for _ in range(self.num_frames)]\n",
    "        \n",
    "        # Uniformly sample frames\n",
    "        indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                # Fallback to last valid frame or black frame\n",
    "                if len(frames) > 0:\n",
    "                    frames.append(frames[-1])\n",
    "                else:\n",
    "                    frames.append(np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video_path = row['video_path']\n",
    "        filename = row['filename']\n",
    "        \n",
    "        # Extract frames\n",
    "        frames = self.extract_frames(video_path)\n",
    "        \n",
    "        # Apply augmentation to each frame\n",
    "        transformed_frames = []\n",
    "        for frame in frames:\n",
    "            augmented = self.transform(image=frame)\n",
    "            transformed_frames.append(augmented['image'])\n",
    "        \n",
    "        # Stack frames: [num_frames, C, H, W]\n",
    "        frames_tensor = torch.stack(transformed_frames)\n",
    "        \n",
    "        return frames_tensor, filename\n",
    "\n",
    "print(\"‚úÖ VideoInferenceDataset class defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b6e31",
   "metadata": {},
   "source": [
    "## üöÄ STEP 8: Run Full Inference with TTA\n",
    "\n",
    "**Process:** For each test video, run inference with 5 TTA variants and average predictions  \n",
    "**Safety:** VRAM monitoring with emergency cache clearing + enhanced OOM fallback  \n",
    "**Estimated time:** ~25-35 minutes for 200 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8cd969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to check VRAM usage\n",
    "def get_vram_usage():\n",
    "    \"\"\"Get current VRAM usage percentage\"\"\"\n",
    "    try:\n",
    "        if gpu_handle is None:\n",
    "            return 0.0\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "        return mem_info.used / mem_info.total\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Main inference function with TTA, cache management, and enhanced fallback\n",
    "def run_inference_with_tta(model, test_dataframe, num_frames, num_tta=5, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Run inference on all test videos with TTA\n",
    "    \n",
    "    Features:\n",
    "    - 5 TTA variants per video\n",
    "    - Average softmax probabilities across TTA\n",
    "    - VRAM monitoring with safety checks\n",
    "    - Enhanced OOM fallback (retry ‚Üí reduce frames ‚Üí default)\n",
    "    - Confidence score = probability of predicted class\n",
    "    \"\"\"\n",
    "    tta_transforms = get_tta_transforms()\n",
    "    all_predictions = {}\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"üöÄ Starting inference on {len(test_dataframe)} videos with {num_tta} TTA variants...\")\n",
    "    print(f\"   Total inference passes: {len(test_dataframe) * num_tta}\")\n",
    "    print(f\"   Estimated time: ~{len(test_dataframe) * num_tta * 0.02:.0f}-{len(test_dataframe) * num_tta * 0.03:.0f} minutes\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(test_dataframe.iterrows(), total=len(test_dataframe), desc=\"Processing videos\"):\n",
    "            filename = row['filename']\n",
    "            video_path = row['video_path']\n",
    "            \n",
    "            tta_softmax_outputs = []\n",
    "            \n",
    "            # Run inference with each TTA variant\n",
    "            for tta_idx in range(num_tta):\n",
    "                # VRAM safety check before each TTA variant\n",
    "                vram_usage = get_vram_usage()\n",
    "                if vram_usage > config.VRAM_EMERGENCY_THRESHOLD:\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # Create dataset for this TTA variant\n",
    "                single_video_df = pd.DataFrame([row])\n",
    "                dataset = VideoInferenceDataset(single_video_df, num_frames, tta_transforms[tta_idx])\n",
    "                loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "                \n",
    "                # Get frames and filename\n",
    "                try:\n",
    "                    frames, _ = next(iter(loader))\n",
    "                    frames = frames.to(device)\n",
    "                    \n",
    "                    # Forward pass with mixed precision\n",
    "                    with autocast(enabled=config.MIXED_PRECISION):\n",
    "                        output = model(frames)\n",
    "                        softmax = F.softmax(output, dim=1)[0]  # [2] - [prob_real, prob_fake]\n",
    "                    \n",
    "                    tta_softmax_outputs.append(softmax.cpu())\n",
    "                \n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        # Enhanced OOM fallback\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                        # Retry 1: Same frames, after cache clear\n",
    "                        try:\n",
    "                            frames = frames.to(device)\n",
    "                            with autocast(enabled=config.MIXED_PRECISION):\n",
    "                                output = model(frames)\n",
    "                                softmax = F.softmax(output, dim=1)[0]\n",
    "                            tta_softmax_outputs.append(softmax.cpu())\n",
    "                        except:\n",
    "                            # Retry 2: Reduce to 16 frames (emergency mode)\n",
    "                            try:\n",
    "                                dataset_reduced = VideoInferenceDataset(\n",
    "                                    single_video_df, 16, tta_transforms[tta_idx]\n",
    "                                )\n",
    "                                loader_reduced = DataLoader(dataset_reduced, batch_size=1, shuffle=False, num_workers=0)\n",
    "                                frames_reduced, _ = next(iter(loader_reduced))\n",
    "                                frames_reduced = frames_reduced.to(device)\n",
    "                                \n",
    "                                with autocast(enabled=config.MIXED_PRECISION):\n",
    "                                    output = model(frames_reduced)\n",
    "                                    softmax = F.softmax(output, dim=1)[0]\n",
    "                                tta_softmax_outputs.append(softmax.cpu())\n",
    "                            except:\n",
    "                                # Ultimate fallback: neutral prediction\n",
    "                                tta_softmax_outputs.append(torch.tensor([0.5, 0.5]))\n",
    "                    else:\n",
    "                        raise e\n",
    "            \n",
    "            # Average softmax across all TTA variants (Method B - standard practice)\n",
    "            avg_softmax = torch.mean(torch.stack(tta_softmax_outputs), dim=0)  # [2]\n",
    "            \n",
    "            # Get final prediction and confidence\n",
    "            final_label = torch.argmax(avg_softmax).item()  # 0 or 1\n",
    "            confidence = torch.max(avg_softmax).item()  # Confidence of predicted class\n",
    "            \n",
    "            all_predictions[filename] = {\n",
    "                'label': final_label,\n",
    "                'probability': confidence\n",
    "            }\n",
    "            \n",
    "            # Regular cache clear after processing all TTA variants for this video\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Inference complete!\")\n",
    "    print(f\"   Total time: {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\"   Average time per video: {elapsed_time/len(test_dataframe):.1f} seconds\")\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "print(\"‚úÖ Inference function defined with TTA + enhanced safety!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute full inference on all 200 test videos\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING FULL INFERENCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Videos: {len(test_df)}\")\n",
    "print(f\"Frames per video: {FRAMES_PER_VIDEO}\")\n",
    "print(f\"TTA variants: 5\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "predictions = run_inference_with_tta(\n",
    "    model=model,\n",
    "    test_dataframe=test_df,\n",
    "    num_frames=FRAMES_PER_VIDEO,\n",
    "    num_tta=5,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(predictions)} predictions!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec14a1",
   "metadata": {},
   "source": [
    "## üíæ STEP 9: Save & Validate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5042b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions dataframe\n",
    "predictions_df = pd.DataFrame([\n",
    "    {\n",
    "        'filename': filename,\n",
    "        'label': pred['label'],\n",
    "        'probability': pred['probability']\n",
    "    }\n",
    "    for filename, pred in predictions.items()\n",
    "])\n",
    "\n",
    "# Sort by filename for consistency\n",
    "predictions_df = predictions_df.sort_values('filename').reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTIONS GENERATED - RUNNING VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ESSENTIAL VALIDATION CHECKS\n",
    "validation_passed = True\n",
    "\n",
    "# Check 1: Exactly 200 rows\n",
    "if len(predictions_df) != 200:\n",
    "    print(f\"‚ùå ERROR: Expected 200 predictions, got {len(predictions_df)}\")\n",
    "    validation_passed = False\n",
    "else:\n",
    "    print(f\"‚úÖ Row count: {len(predictions_df)}\")\n",
    "\n",
    "# Check 2: All labels are 0 or 1\n",
    "invalid_labels = predictions_df[~predictions_df['label'].isin([0, 1])]\n",
    "if len(invalid_labels) > 0:\n",
    "    print(f\"‚ùå ERROR: {len(invalid_labels)} invalid labels found\")\n",
    "    validation_passed = False\n",
    "else:\n",
    "    print(f\"‚úÖ All labels valid (0 or 1)\")\n",
    "\n",
    "# Check 3: All probabilities between 0 and 1\n",
    "invalid_probs = predictions_df[(predictions_df['probability'] < 0) | (predictions_df['probability'] > 1)]\n",
    "if len(invalid_probs) > 0:\n",
    "    print(f\"‚ùå ERROR: {len(invalid_probs)} probabilities out of range [0, 1]\")\n",
    "    validation_passed = False\n",
    "else:\n",
    "    print(f\"‚úÖ All probabilities in valid range [0, 1]\")\n",
    "\n",
    "# Check 4: All filenames match test_public.csv\n",
    "missing_files = set(test_df['filename']) - set(predictions_df['filename'])\n",
    "extra_files = set(predictions_df['filename']) - set(test_df['filename'])\n",
    "if missing_files or extra_files:\n",
    "    print(f\"‚ùå ERROR: Filename mismatch!\")\n",
    "    if missing_files:\n",
    "        print(f\"   Missing: {list(missing_files)[:5]}\")\n",
    "    if extra_files:\n",
    "        print(f\"   Extra: {list(extra_files)[:5]}\")\n",
    "    validation_passed = False\n",
    "else:\n",
    "    print(f\"‚úÖ All filenames match test_public.csv\")\n",
    "\n",
    "if not validation_passed:\n",
    "    raise ValueError(\"‚ùå Validation failed! Check errors above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display statistics\n",
    "real_count = (predictions_df['label'] == 0).sum()\n",
    "fake_count = (predictions_df['label'] == 1).sum()\n",
    "avg_confidence = predictions_df['probability'].mean()\n",
    "\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(f\"Predicted Real (0): {real_count} ({real_count/len(predictions_df)*100:.1f}%)\")\n",
    "print(f\"Predicted Fake (1): {fake_count} ({fake_count/len(predictions_df)*100:.1f}%)\")\n",
    "print(f\"Average confidence: {avg_confidence:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL VALIDATIONS PASSED!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faadf527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "predictions_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTIONS SAVED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÅ Output file: {OUTPUT_CSV}\")\n",
    "print(f\"   File size: {OUTPUT_CSV.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"   Format: filename, label, probability\")\n",
    "\n",
    "# Verify CSV can be read back\n",
    "verify_df = pd.read_csv(OUTPUT_CSV)\n",
    "print(f\"\\n‚úÖ CSV verification:\")\n",
    "print(f\"   Rows: {len(verify_df)}\")\n",
    "print(f\"   Columns: {list(verify_df.columns)}\")\n",
    "\n",
    "# Display first 10 predictions as sample\n",
    "print(f\"\\nüìã First 10 predictions:\")\n",
    "print(verify_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ INFERENCE PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüéâ SUCCESS! Generated {len(verify_df)} predictions\")\n",
    "print(f\"üìÅ Output: {OUTPUT_CSV}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Review predictions above\")\n",
    "print(f\"  2. Check {OUTPUT_DIR} folder\")\n",
    "print(f\"  3. Submit PREDICTIONS.CSV to competition\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}