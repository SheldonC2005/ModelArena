{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8468558f",
   "metadata": {},
   "source": [
    "# üéØ DEEPFAKE DETECTION - INCEPTIONRESNETV2 + ATTENTION (LOCAL GPU)\n",
    "\n",
    "**Mission:** Train single InceptionResNetV2 + Attention Pooling model for deepfake detection\n",
    "\n",
    "**Architecture:**\n",
    "- CNN: InceptionResNetV2 (pretrained)\n",
    "- Temporal: Attention Pooling (learns to focus on suspicious frames)\n",
    "- Classifier: FC layers with dropout\n",
    "\n",
    "**Training Strategy:**\n",
    "- 70:30 Train/Validation Split (420/180 videos)\n",
    "- Gradient Accumulation (simulates batch_size=4)\n",
    "- Mixed precision training (FP16)\n",
    "- Hyperparameter tuning with Optuna (LR, attention_dim, fc_dropout)\n",
    "- **Hardware:** RTX 3050 Laptop (4GB VRAM)\n",
    "- **GPU Safety:** Thermal monitoring, pause at 78¬∞C\n",
    "- **Data Leak Prevention:** Clean split before Optuna\n",
    "\n",
    "**Expected Runtime:** ~6-8 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üìã SESSION TRACKING\n",
    "Mark your progress:\n",
    "- [ ] Environment setup complete\n",
    "- [ ] Optuna hyperparameter tuning complete\n",
    "- [ ] Training started\n",
    "- [ ] Model training complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde53b4",
   "metadata": {},
   "source": [
    "## üîß STEP 1: Environment Setup & GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4450e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and system info\n",
    "import torch\n",
    "import platform\n",
    "print(f\"System: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå ERROR: GPU not detected! Check CUDA installation.\")\n",
    "    raise RuntimeError(\"GPU required for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required packages are installed\n",
    "# (Packages should already be installed from requirements-local.txt)\n",
    "try:\n",
    "    import cv2\n",
    "    import albumentations\n",
    "    import timm\n",
    "    import pynvml\n",
    "    print(\"‚úÖ All required packages verified!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing package{e}\")\n",
    "    print(\"Please run: pip install -r requirements-local.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "# Additional libraries\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import timm\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import signal\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# GPU Monitoring & Hardware Safety\n",
    "import pynvml\n",
    "import psutil\n",
    "\n",
    "# Initialize NVML for GPU monitoring\n",
    "try:\n",
    "    pynvml.nvmlInit()\n",
    "    print(\"‚úÖ NVML initialized for GPU monitoring\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è NVML initialization failed: {e}\")\n",
    "    print(\"Thermal monitoring may be limited\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Libraries imported and seed set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97775b3e",
   "metadata": {},
   "source": [
    "## üíæ STEP 2: Setup Local Paths & Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup local paths (Windows)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory - adjust if needed\n",
    "BASE_DIR = Path(r\"D:\\Data\\Github\\SheldonC2005\\ModelArena\")\n",
    "BASE_PATH = BASE_DIR / \"archive\"\n",
    "TRAIN_FAKE_PATH = BASE_PATH / \"train\" / \"fake\"\n",
    "TRAIN_REAL_PATH = BASE_PATH / \"train\" / \"real\"\n",
    "TEST_PATH = BASE_PATH / \"test\"\n",
    "LABELS_PATH = BASE_PATH / \"train_labels.csv\"\n",
    "TEST_CSV_PATH = BASE_PATH / \"test_public.csv\"\n",
    "\n",
    "# Output directories\n",
    "SAVE_PATH = BASE_DIR / \"models\"\n",
    "LOG_PATH = BASE_DIR / \"logs\"\n",
    "SAVE_PATH.mkdir(exist_ok=True, parents=True)\n",
    "LOG_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"‚úÖ Working directory: {BASE_DIR}\")\n",
    "print(f\"‚úÖ Archive path: {BASE_PATH}\")\n",
    "print(f\"‚úÖ Models will be saved to: {SAVE_PATH}\")\n",
    "print(f\"‚úÖ Logs will be saved to: {LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "print(\"üìÅ Dataset Structure Verification:\")\n",
    "print(f\"Train Fake videos: {len(list(TRAIN_FAKE_PATH.glob('*.mp4')))} files\")\n",
    "print(f\"Train Real videos: {len(list(TRAIN_REAL_PATH.glob('*.mp4')))} files\")\n",
    "print(f\"Test videos: {len(list(TEST_PATH.glob('*.mp4')))} files\")\n",
    "print(f\"Labels CSV exists: {LABELS_PATH.exists()}\")\n",
    "print(f\"Test CSV exists: {TEST_CSV_PATH.exists()}\")\n",
    "\n",
    "# Check if counts match expected\n",
    "assert len(list(TRAIN_FAKE_PATH.glob('*.mp4'))) == 300, \"‚ùå Expected 300 fake videos!\"\n",
    "assert len(list(TRAIN_REAL_PATH.glob('*.mp4'))) == 300, \"‚ùå Expected 300 real videos!\"\n",
    "assert len(list(TEST_PATH.glob('*.mp4'))) == 200, \"‚ùå Expected 200 test videos!\"\n",
    "print(\"\\n‚úÖ Dataset verified successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c092cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Monitoring & Hardware Safety Functions\n",
    "class GPUMonitor:\n",
    "    \"\"\"Monitor GPU temperature and VRAM usage for hardware safety\"\"\"\n",
    "    \n",
    "    def __init__(self, temp_threshold=78, temp_resume=72, vram_threshold=0.90):\n",
    "        self.temp_threshold = temp_threshold  # Pause training at this temp\n",
    "        self.temp_resume = temp_resume  # Resume when temp drops to this\n",
    "        self.vram_threshold = vram_threshold  # Warn at 90% VRAM usage\n",
    "        self.gpu_handle = None\n",
    "        \n",
    "        try:\n",
    "            self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            print(f\"‚úÖ GPU Monitor initialized\")\n",
    "            print(f\"   Thermal protection: Pause at {temp_threshold}¬∞C, resume at {temp_resume}¬∞C\")\n",
    "            print(f\"   VRAM protection: Warning at {vram_threshold*100}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è GPU monitoring limited: {e}\")\n",
    "    \n",
    "    def get_gpu_temp(self):\n",
    "        \"\"\"Get current GPU temperature\"\"\"\n",
    "        try:\n",
    "            if self.gpu_handle is None:\n",
    "                return 0\n",
    "            temp = pynvml.nvmlDeviceGetTemperature(self.gpu_handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "            return temp\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def get_vram_usage(self):\n",
    "        \"\"\"Get current VRAM usage percentage\"\"\"\n",
    "        try:\n",
    "            if self.gpu_handle is None:\n",
    "                return 0.0\n",
    "            mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)\n",
    "            return mem_info.used / mem_info.total\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def check_thermal_safety(self):\n",
    "        \"\"\"Check if GPU temperature is safe\"\"\"\n",
    "        temp = self.get_gpu_temp()\n",
    "        \n",
    "        # If monitoring is unavailable (temp=0), assume safe\n",
    "        if temp == 0 and self.gpu_handle is None:\n",
    "            return True, 0\n",
    "        \n",
    "        if temp >= self.temp_threshold:\n",
    "            return False, temp\n",
    "        return True, temp\n",
    "    \n",
    "    def wait_for_cooling(self):\n",
    "        \"\"\"Wait for GPU to cool down\"\"\"\n",
    "        print(f\"\\nüî• GPU overheating! Pausing training...\")\n",
    "        while True:\n",
    "            temp = self.get_gpu_temp()\n",
    "            print(f\"   Current temp: {temp}¬∞C. Waiting for {self.temp_resume}¬∞C...\", end='\\r')\n",
    "            if temp <= self.temp_resume:\n",
    "                print(f\"\\n‚úÖ GPU cooled to {temp}¬∞C. Resuming training...\")\n",
    "                break\n",
    "            time.sleep(30)  # Check every 30 seconds\n",
    "    \n",
    "    def check_vram_safety(self):\n",
    "        \"\"\"Check VRAM usage\"\"\"\n",
    "        usage = self.get_vram_usage()\n",
    "        if usage > self.vram_threshold:\n",
    "            print(f\"‚ö†Ô∏è VRAM usage high: {usage*100:.1f}%\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "# Initialize GPU monitor\n",
    "gpu_monitor = GPUMonitor(temp_threshold=78, temp_resume=72, vram_threshold=0.90)\n",
    "\n",
    "# Setup logging\n",
    "log_file = LOG_PATH / f\"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"TRAINING SESSION STARTED\")\n",
    "logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "logger.info(f\"Log file: {log_file}\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419df209",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è STEP 3: Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8496413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class - Optimized for RTX 3050 (4GB VRAM)\n",
    "class Config:\n",
    "    # Model configuration - InceptionResNetV2 only\n",
    "    MODEL_NAME = 'inception_resnet_v2_best.pt'\n",
    "    IMG_SIZE = 299\n",
    "    FEATURE_DIM = 1536\n",
    "    \n",
    "    # Training hyperparameters - OPTIMIZED FOR 4GB VRAM\n",
    "    FRAMES_PER_VIDEO = 24\n",
    "    BATCH_SIZE = 1  # Safe for 4GB VRAM with InceptionResNetV2\n",
    "    ACCUMULATION_STEPS = 4  # Simulates batch_size=4\n",
    "    NUM_EPOCHS = 30\n",
    "    EARLY_STOP_PATIENCE = 3\n",
    "    TRAIN_VAL_SPLIT = 0.7  # 70% train, 30% validation\n",
    "    \n",
    "    # Attention parameters (will be tuned by Optuna)\n",
    "    ATTENTION_DIM = 256  # Default, will be tuned\n",
    "    \n",
    "    # Optimizer parameters (will be tuned by Optuna)\n",
    "    LEARNING_RATE = 1e-4  # Default, will be tuned\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    FC_DROPOUT = 0.5  # Default, will be tuned\n",
    "    \n",
    "    # Other settings - OPTIMIZED FOR LOCAL GPU\n",
    "    NUM_WORKERS = 0  # Safe for Windows\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    MIXED_PRECISION = True  # Critical for 4GB VRAM\n",
    "    \n",
    "    # Optuna hyperparameter search\n",
    "    OPTUNA_N_TRIALS = 10\n",
    "    OPTUNA_TIMEOUT = 1800  # 30 minutes\n",
    "    \n",
    "    # Hardware safety settings\n",
    "    CHECK_GPU_TEMP_EVERY_N_BATCHES = 50\n",
    "    ENABLE_THERMAL_MONITORING = True\n",
    "    ENABLE_VRAM_MONITORING = True\n",
    "\n",
    "config = Config()\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"CONFIGURATION LOADED - INCEPTIONRESNETV2 + ATTENTION\")\n",
    "logger.info(f\"Device: {config.DEVICE}\")\n",
    "logger.info(f\"Model: {config.MODEL_NAME}\")\n",
    "logger.info(f\"Batch Size: {config.BATCH_SIZE} (with accumulation_steps={config.ACCUMULATION_STEPS})\")\n",
    "logger.info(f\"Frames per video: {config.FRAMES_PER_VIDEO}\")\n",
    "logger.info(f\"Train/Val Split: {int(config.TRAIN_VAL_SPLIT*100)}/{int((1-config.TRAIN_VAL_SPLIT)*100)}\")\n",
    "logger.info(f\"Mixed Precision: {config.MIXED_PRECISION}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "print(f\"Model: InceptionResNetV2 + Attention Pooling\")\n",
    "print(f\"Effective Batch Size: {config.BATCH_SIZE * config.ACCUMULATION_STEPS} (via gradient accumulation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71a4d2",
   "metadata": {},
   "source": [
    "## üìä STEP 4: Data Preprocessing & Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "print(f\"Total training samples: {len(labels_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "\n",
    "# Validate columns\n",
    "assert 'filename' in labels_df.columns, \"‚ùå 'filename' column missing in train_labels.csv!\"\n",
    "assert 'label' in labels_df.columns, \"‚ùå 'label' column missing in train_labels.csv!\"\n",
    "assert 'filename' in test_df.columns, \"‚ùå 'filename' column missing in test_public.csv!\"\n",
    "\n",
    "print(\"üìä Label Distribution:\")\n",
    "print(labels_df['label'].value_counts())\n",
    "print(f\"\\nLabel 0 (Real): {(labels_df['label']==0).sum()}\")\n",
    "print(f\"Label 1 (Fake): {(labels_df['label']==1).sum()}\")\n",
    "\n",
    "# Create full paths\n",
    "def get_video_path(filename, label):\n",
    "    if label == 1:  # Fake\n",
    "        return str(TRAIN_FAKE_PATH / filename)\n",
    "    else:  # Real\n",
    "        return str(TRAIN_REAL_PATH / filename)\n",
    "\n",
    "labels_df['video_path'] = labels_df.apply(\n",
    "    lambda row: get_video_path(row['filename'], row['label']), axis=1\n",
    ")\n",
    "\n",
    "test_df['video_path'] = test_df['filename'].apply(\n",
    "    lambda x: str(TEST_PATH / x)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Labels prepared: {len(labels_df)} training videos\")\n",
    "print(f\"‚úÖ Test data prepared: {len(test_df)} test videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentations for InceptionResNetV2\n",
    "def get_augmentations(img_size=299, is_train=True):\n",
    "    \"\"\"\n",
    "    Augmentations optimized for InceptionResNetV2 deepfake detection\n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=0, p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Inception normalization\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "print(\"‚úÖ Augmentation functions defined for InceptionResNetV2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Dataset with on-the-fly frame extraction\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_size=299, num_frames=24, is_train=True, has_labels=True):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_size = img_size\n",
    "        self.num_frames = num_frames\n",
    "        self.is_train = is_train\n",
    "        self.has_labels = has_labels\n",
    "        self.transform = get_augmentations(img_size, is_train)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def extract_frames(self, video_path):\n",
    "        \"\"\"\n",
    "        Extract uniformly spaced frames from video\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Handle corrupted or empty videos\n",
    "        if total_frames <= 0:\n",
    "            cap.release()\n",
    "            # Return black frames as fallback\n",
    "            return [np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8) for _ in range(self.num_frames)]\n",
    "        \n",
    "        if total_frames < self.num_frames:\n",
    "            # If video has fewer frames, repeat some frames\n",
    "            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        else:\n",
    "            # Uniformly sample frames\n",
    "            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                # If frame read fails, use last valid frame or black frame\n",
    "                if len(frames) > 0:\n",
    "                    frames.append(frames[-1])\n",
    "                else:\n",
    "                    frames.append(np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video_path = row['video_path']\n",
    "        \n",
    "        # Extract frames\n",
    "        frames = self.extract_frames(video_path)\n",
    "        \n",
    "        # Apply augmentations to each frame\n",
    "        transformed_frames = []\n",
    "        for frame in frames:\n",
    "            augmented = self.transform(image=frame)\n",
    "            transformed_frames.append(augmented['image'])\n",
    "        \n",
    "        # Stack frames: [num_frames, C, H, W]\n",
    "        frames_tensor = torch.stack(transformed_frames)\n",
    "        \n",
    "        if self.has_labels:\n",
    "            label = torch.tensor(row['label'], dtype=torch.long)\n",
    "            return frames_tensor, label\n",
    "        else:\n",
    "            return frames_tensor\n",
    "\n",
    "print(\"‚úÖ VideoDataset class defined with on-the-fly frame extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d29e43",
   "metadata": {},
   "source": [
    "## üèóÔ∏è STEP 5: Model Architecture (CNN + Attention Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fddd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Attention_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    InceptionResNetV2 (Feature Extractor) + Attention Pooling (Temporal Modeling) for video classification\n",
    "    Attention learns to focus on \"suspicious\" frames for deepfake detection\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=1536, attention_dim=256, fc_dropout=0.5, num_classes=2):\n",
    "        super(CNN_Attention_Model, self).__init__()\n",
    "        \n",
    "        # Load pretrained InceptionResNetV2\n",
    "        self.cnn = timm.create_model('inception_resnet_v2', pretrained=True, num_classes=0)\n",
    "        \n",
    "        # Attention mechanism for temporal modeling\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, attention_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fc_dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_frames, C, H, W]\n",
    "        batch_size, num_frames, C, H, W = x.shape\n",
    "        \n",
    "        # Reshape to process all frames: [batch_size * num_frames, C, H, W]\n",
    "        x = x.view(batch_size * num_frames, C, H, W)\n",
    "        \n",
    "        # Extract features from CNN\n",
    "        features = self.cnn(x)  # [batch_size * num_frames, feature_dim]\n",
    "        \n",
    "        # Reshape back to sequence: [batch_size, num_frames, feature_dim]\n",
    "        features = features.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attn_scores = self.attention(features)  # [batch_size, num_frames, 1]\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)  # [batch_size, num_frames, 1]\n",
    "        \n",
    "        # Weighted sum of features\n",
    "        context = torch.sum(features * attn_weights, dim=1)  # [batch_size, feature_dim]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.fc(context)  # [batch_size, num_classes]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ InceptionResNetV2 + Attention Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model instantiation\n",
    "print(\"üß™ Testing InceptionResNetV2 + Attention model...\\n\")\n",
    "\n",
    "model = CNN_Attention_Model(\n",
    "    feature_dim=config.FEATURE_DIM,\n",
    "    attention_dim=config.ATTENTION_DIM,\n",
    "    fc_dropout=config.FC_DROPOUT\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, config.FRAMES_PER_VIDEO, 3, config.IMG_SIZE, config.IMG_SIZE)\n",
    "output = model(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\n‚úÖ Model working correctly!\\n\")\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6730e",
   "metadata": {},
   "source": [
    "## üîç STEP 6: Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3fd04d",
   "metadata": {},
   "source": [
    "## üìä STEP 6A: Create 70:30 Train/Validation Split (BEFORE Optuna)\n",
    "\n",
    "**CRITICAL: Split data FIRST to prevent any possibility of data leak**\n",
    "- Creates train_df (420 videos, 70%)\n",
    "- Creates val_df (180 videos, 30%)\n",
    "- Optuna will ONLY use train_df for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 70:30 train/validation split BEFORE Optuna\n",
    "# This ensures Optuna ONLY sees training data, preventing any data leak\n",
    "print(\"üìä Creating 70:30 Train/Validation Split...\")\n",
    "print(f\"Total videos: {len(labels_df)}\")\n",
    "print(f\"Train split: {config.TRAIN_VAL_SPLIT*100:.0f}%\")\n",
    "print(f\"Val split: {(1-config.TRAIN_VAL_SPLIT)*100:.0f}%\\n\")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    labels_df,\n",
    "    train_size=config.TRAIN_VAL_SPLIT,\n",
    "    stratify=labels_df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display split statistics\n",
    "print(f\"‚úÖ Split created successfully!\")\n",
    "print(f\"\\nTrain set: {len(train_df)} videos\")\n",
    "print(f\"  - Real: {len(train_df[train_df['label']==0])}\")\n",
    "print(f\"  - Fake: {len(train_df[train_df['label']==1])}\")\n",
    "\n",
    "print(f\"\\nValidation set: {len(val_df)} videos\")\n",
    "print(f\"  - Real: {len(val_df[val_df['label']==0])}\")\n",
    "print(f\"  - Fake: {len(val_df[val_df['label']==1])}\")\n",
    "\n",
    "print(f\"\\nClass balance preserved: {len(train_df[train_df['label']==1])/len(train_df)*100:.1f}% fake in train, {len(val_df[val_df['label']==1])/len(val_df)*100:.1f}% fake in val\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚ö†Ô∏è IMPORTANT: Optuna will ONLY use train_df (420 videos)\")\n",
    "print(\"             Validation data (180 videos) will NOT be touched by Optuna\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc7cec",
   "metadata": {},
   "source": [
    "## üîç STEP 6B: Hyperparameter Tuning with Optuna (Using train_df ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with gradient accumulation and hardware safety monitoring\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, scaler, device, epoch_num=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    batch_count = 0\n",
    "    accumulation_step = 0  # Track gradient accumulation\n",
    "    \n",
    "    # Initialize gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Training Epoch {epoch_num+1}')\n",
    "    \n",
    "    for batch_idx, (frames, labels) in enumerate(pbar):\n",
    "        # Thermal safety check every N batches\n",
    "        if batch_idx % config.CHECK_GPU_TEMP_EVERY_N_BATCHES == 0 and config.ENABLE_THERMAL_MONITORING:\n",
    "            is_safe, temp = gpu_monitor.check_thermal_safety()\n",
    "            if not is_safe:\n",
    "                logger.warning(f\"GPU temperature {temp}¬∞C exceeds threshold. Pausing...\")\n",
    "                gpu_monitor.wait_for_cooling()\n",
    "        \n",
    "        # VRAM safety check\n",
    "        if config.ENABLE_VRAM_MONITORING and batch_idx % 20 == 0:\n",
    "            if not gpu_monitor.check_vram_safety():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        try:\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with autocast(enabled=config.MIXED_PRECISION):\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Normalize loss for gradient accumulation\n",
    "                loss = loss / config.ACCUMULATION_STEPS\n",
    "            \n",
    "            # Backward pass - accumulate gradients\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            accumulation_step += 1\n",
    "            \n",
    "            # Update weights only every ACCUMULATION_STEPS batches\n",
    "            if accumulation_step % config.ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Track metrics (use original loss scale for logging)\n",
    "            running_loss += loss.item() * config.ACCUMULATION_STEPS\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            batch_count += 1\n",
    "            pbar.set_postfix({\n",
    "                'loss': loss.item() * config.ACCUMULATION_STEPS, \n",
    "                'accum': f\"{accumulation_step % config.ACCUMULATION_STEPS}/{config.ACCUMULATION_STEPS}\",\n",
    "                'temp': f\"{gpu_monitor.get_gpu_temp()}¬∞C\"\n",
    "            })\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                logger.error(f\"OOM Error in batch {batch_idx}. Clearing cache and skipping batch.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                # Reset gradients on OOM to avoid corrupted state\n",
    "                optimizer.zero_grad()\n",
    "                accumulation_step = 0\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # Final optimizer step if there are remaining accumulated gradients\n",
    "    if accumulation_step % config.ACCUMULATION_STEPS != 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    epoch_loss = running_loss / max(batch_count, 1)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    batch_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for batch_idx, (frames, labels) in enumerate(pbar):\n",
    "            try:\n",
    "                frames, labels = frames.to(device), labels.to(device)\n",
    "                \n",
    "                with autocast(enabled=config.MIXED_PRECISION):\n",
    "                    outputs = model(frames)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "                \n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    logger.error(f\"OOM Error in validation batch {batch_idx}. Clearing cache and skipping.\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    # Handle edge case of all batches failing\n",
    "    if batch_count == 0 or len(all_labels) == 0:\n",
    "        logger.error(\"No valid validation batches processed!\")\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    epoch_loss = running_loss / max(batch_count, 1)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    epoch_auc = roc_auc_score(all_labels, all_probs)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, epoch_f1, epoch_auc\n",
    "\n",
    "# Graceful shutdown handler\n",
    "def signal_handler(sig, frame):\n",
    "    logger.info(\"\\nüõë Ctrl+C detected. Saving checkpoint and exiting gracefully...\")\n",
    "    print(\"\\nüõë Training interrupted. Checkpoint saved. You can resume later.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# Register signal handler\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "print(\"‚úÖ Training and validation functions defined with hardware safety features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective function for hyperparameter tuning\n",
    "def optuna_objective(trial):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters using InceptionResNetV2 + Attention\n",
    "    DATA LEAK PREVENTION: Uses ONLY train_df (already split), validation data never touched\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    attention_dim = trial.suggest_categorical('attention_dim', [128, 256, 512])\n",
    "    fc_dropout = trial.suggest_float('fc_dropout', 0.3, 0.7)\n",
    "    \n",
    "    # Use 20% of train_df for Optuna tuning (~84 videos from 420 training videos)\n",
    "    # val_df (180 videos) is NEVER touched by Optuna\n",
    "    tune_df = train_df.sample(frac=0.2, random_state=42)\n",
    "    \n",
    "    # Split Optuna data into train/val (80/20 split = 67/17 videos)\n",
    "    tune_train_df = tune_df.iloc[:int(len(tune_df)*0.8)]\n",
    "    tune_val_df = tune_df.iloc[int(len(tune_df)*0.8):]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = VideoDataset(\n",
    "        tune_train_df, config.IMG_SIZE, \n",
    "        config.FRAMES_PER_VIDEO, is_train=True\n",
    "    )\n",
    "    val_dataset = VideoDataset(\n",
    "        tune_val_df, config.IMG_SIZE, \n",
    "        config.FRAMES_PER_VIDEO, is_train=False\n",
    "    )\n",
    "    \n",
    "    # Use num_workers=0 to avoid multiprocessing issues on Windows\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, \n",
    "                             shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, \n",
    "                           shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Create model with tuned parameters\n",
    "    model = CNN_Attention_Model(\n",
    "        feature_dim=config.FEATURE_DIM,\n",
    "        attention_dim=attention_dim,\n",
    "        fc_dropout=fc_dropout\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=config.WEIGHT_DECAY)\n",
    "    scaler = GradScaler(enabled=config.MIXED_PRECISION)\n",
    "    \n",
    "    # Train for 5 epochs (quick tuning)\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, config.DEVICE, epoch_num=epoch)\n",
    "        val_loss, val_acc, val_f1, val_auc = validate(model, val_loader, criterion, config.DEVICE)\n",
    "        \n",
    "        # Report intermediate value\n",
    "        trial.report(val_acc, epoch)\n",
    "        \n",
    "        # Prune trial if not promising\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "    \n",
    "    return best_val_acc\n",
    "\n",
    "print(\"‚úÖ Optuna objective function defined with data leak prevention!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d45d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter tuning\n",
    "print(\"üîç Starting hyperparameter tuning with Optuna...\")\n",
    "print(f\"Number of trials: {config.OPTUNA_N_TRIALS}\")\n",
    "print(f\"Timeout: {config.OPTUNA_TIMEOUT} seconds ({config.OPTUNA_TIMEOUT/60:.1f} minutes)\\n\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
    ")\n",
    "\n",
    "study.optimize(optuna_objective, n_trials=config.OPTUNA_N_TRIALS, timeout=config.OPTUNA_TIMEOUT)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèÜ BEST HYPERPARAMETERS FOUND:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best validation accuracy: {study.best_value:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Update config with best parameters\n",
    "config.LEARNING_RATE = best_params['learning_rate']\n",
    "config.ATTENTION_DIM = best_params['attention_dim']\n",
    "config.FC_DROPOUT = best_params['fc_dropout']\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameters optimized and updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training function with early stopping\n",
    "def train_model_full(train_df, val_df):\n",
    "    \"\"\"\n",
    "    Train InceptionResNetV2 + Attention model on 70:30 split\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training InceptionResNetV2 + Attention Pooling\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = VideoDataset(\n",
    "        train_df, config.IMG_SIZE, \n",
    "        config.FRAMES_PER_VIDEO, is_train=True\n",
    "    )\n",
    "    val_dataset = VideoDataset(\n",
    "        val_df, config.IMG_SIZE, \n",
    "        config.FRAMES_PER_VIDEO, is_train=False\n",
    "    )\n",
    "    \n",
    "    # Use num_workers=0 to avoid Windows multiprocessing issues\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.BATCH_SIZE, \n",
    "        shuffle=True, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=config.BATCH_SIZE, \n",
    "        shuffle=False, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create model with optimized hyperparameters from Optuna\n",
    "    model = CNN_Attention_Model(\n",
    "        feature_dim=config.FEATURE_DIM,\n",
    "        attention_dim=config.ATTENTION_DIM,\n",
    "        fc_dropout=config.FC_DROPOUT\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, \n",
    "                                  weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    scaler = GradScaler(enabled=config.MIXED_PRECISION)\n",
    "    \n",
    "    # Training tracking\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = None\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_auc': []}\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, config.DEVICE, epoch_num=epoch)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, val_f1, val_auc = validate(model, val_loader, criterion, config.DEVICE)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Early stopping and model saving\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"‚úÖ New best model! Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_path = SAVE_PATH / config.MODEL_NAME\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model_wts,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': best_val_acc,\n",
    "                'val_f1': val_f1,\n",
    "                'val_auc': val_auc,\n",
    "                'history': history,\n",
    "                'config': {\n",
    "                    'model_name': config.MODEL_NAME,\n",
    "                    'feature_dim': config.FEATURE_DIM,\n",
    "                    'attention_dim': config.ATTENTION_DIM,\n",
    "                    'fc_dropout': config.FC_DROPOUT\n",
    "                }\n",
    "            }, checkpoint_path)\n",
    "            print(f\"üíæ Model saved to {checkpoint_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience: {patience_counter}/{config.EARLY_STOP_PATIENCE}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config.EARLY_STOP_PATIENCE:\n",
    "            print(f\"\\n‚èπÔ∏è Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ InceptionResNetV2 + Attention Training Complete!\")\n",
    "    print(f\"Best Val Accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return model, best_val_acc, history\n",
    "\n",
    "print(\"‚úÖ Full training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84c7db",
   "metadata": {},
   "source": [
    "## üöÄ STEP 7: Train Model\n",
    "\n",
    "**Training Configuration:**\n",
    "- Model: InceptionResNetV2 + Attention Pooling\n",
    "- Batch Size: 1 (with gradient accumulation √ó 4)\n",
    "- Mixed Precision: FP16\n",
    "- Hardware: RTX 3050 Laptop (4GB VRAM)\n",
    "- **Data:** Using train_df (420 videos) and val_df (180 videos) from STEP 6A\n",
    "\n",
    "**Estimated Time:** ~6-8 hours on RTX 3050 Laptop GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train InceptionResNetV2 + Attention model\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Model: InceptionResNetV2 + Attention Pooling\")\n",
    "print(f\"Hyperparameters: LR={config.LEARNING_RATE:.6f}, Attention_Dim={config.ATTENTION_DIM}, FC_Dropout={config.FC_DROPOUT}\\n\")\n",
    "\n",
    "# Record training start time\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Train model\n",
    "model, best_val_acc, history = train_model_full(train_df, val_df)\n",
    "\n",
    "# Calculate training time\n",
    "training_duration = time.time() - training_start_time\n",
    "hours = int(training_duration // 3600)\n",
    "minutes = int((training_duration % 3600) // 60)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üéâ TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Training time: {hours}h {minutes}m\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Model saved to: {SAVE_PATH / config.MODEL_NAME}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03943aac",
   "metadata": {},
   "source": [
    "## üìà STEP 8: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('InceptionResNetV2 + Attention - Training Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Training and Validation Accuracy\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['train_acc'], label='Training Accuracy', linewidth=2, color='#2E86AB')\n",
    "ax.plot(history['val_acc'], label='Validation Accuracy', linewidth=2, color='#A23B72', linestyle='--')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Accuracy Curves', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training and Validation Loss\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['train_loss'], label='Training Loss', linewidth=2, color='#2E86AB')\n",
    "ax.plot(history['val_loss'], label='Validation Loss', linewidth=2, color='#A23B72', linestyle='--')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Loss Curves', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Validation F1 Score\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history['val_f1'], label='Validation F1', linewidth=2, color='#F18F01')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title('F1 Score Over Time', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Validation AUC\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history['val_auc'], label='Validation AUC', linewidth=2, color='#C73E1D')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('AUC', fontsize=12)\n",
    "ax.set_title('AUC Over Time', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_PATH / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to: {SAVE_PATH / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FINAL TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best epoch\n",
    "best_epoch = history['val_acc'].index(max(history['val_acc'])) + 1\n",
    "best_metrics = {\n",
    "    'Best Epoch': best_epoch,\n",
    "    'Best Val Accuracy': max(history['val_acc']),\n",
    "    'Val F1 Score': history['val_f1'][best_epoch-1],\n",
    "    'Val AUC': history['val_auc'][best_epoch-1],\n",
    "    'Train Accuracy': history['train_acc'][best_epoch-1],\n",
    "    'Train Loss': history['train_loss'][best_epoch-1],\n",
    "    'Val Loss': history['val_loss'][best_epoch-1]\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nModel: InceptionResNetV2 + Attention Pooling\")\n",
    "print(f\"Training Duration: {hours}h {minutes}m\")\n",
    "print(f\"\\nBest Performance (Epoch {best_epoch}):\")\n",
    "print(f\"  {'Metric':<20} {'Value':>10}\")\n",
    "print(f\"  {'-'*30}\")\n",
    "for metric, value in best_metrics.items():\n",
    "    if metric == 'Best Epoch':\n",
    "        print(f\"  {metric:<20} {value:>10}\")\n",
    "    else:\n",
    "        print(f\"  {metric:<20} {value:>10.4f}\")\n",
    "\n",
    "print(f\"\\nModel saved to: {SAVE_PATH / config.MODEL_NAME}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save summary to JSON\n",
    "import json\n",
    "summary_dict = {\n",
    "    'model': 'InceptionResNetV2 + Attention',\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_duration_hours': round(training_duration / 3600, 2),\n",
    "    'hyperparameters': {\n",
    "        'learning_rate': config.LEARNING_RATE,\n",
    "        'attention_dim': config.ATTENTION_DIM,\n",
    "        'fc_dropout': config.FC_DROPOUT,\n",
    "        'batch_size': config.BATCH_SIZE,\n",
    "        'accumulation_steps': config.ACCUMULATION_STEPS,\n",
    "        'num_epochs': config.NUM_EPOCHS,\n",
    "        'frames_per_video': config.FRAMES_PER_VIDEO,\n",
    "        'img_size': config.IMG_SIZE\n",
    "    },\n",
    "    'best_metrics': best_metrics,\n",
    "    'history': {\n",
    "        'train_loss': [float(x) for x in history['train_loss']],\n",
    "        'train_acc': [float(x) for x in history['train_acc']],\n",
    "        'val_loss': [float(x) for x in history['val_loss']],\n",
    "        'val_acc': [float(x) for x in history['val_acc']],\n",
    "        'val_f1': [float(x) for x in history['val_f1']],\n",
    "        'val_auc': [float(x) for x in history['val_auc']]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(SAVE_PATH / 'training_summary.json', 'w') as f:\n",
    "    json.dump(summary_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Summary saved to: {SAVE_PATH / 'training_summary.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e87dd2",
   "metadata": {},
   "source": [
    "## ‚úÖ Training Complete!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Review training curves and metrics above\n",
    "2. Run `INFERENCE_PIPELINE.ipynb` to make predictions on test data\n",
    "3. Model is saved at: `models/inception_resnet_v2_best.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193898e",
   "metadata": {},
   "source": [
    "## ‚úÖ TRAINING COMPLETE!\n",
    "\n",
    "### üì¶ Saved Files:\n",
    "- `inception_resnet_v2_best.pt` - Best model checkpoint\n",
    "- `training_curves.png` - Training visualization (4 panels: Accuracy, Loss, F1, AUC)\n",
    "- `training_summary.json` - Detailed metrics and history\n",
    "\n",
    "### üîÑ Next Steps:\n",
    "1. Review training curves and metrics above\n",
    "2. Open `INFERENCE_PIPELINE.ipynb`  \n",
    "3. Load the trained model\n",
    "4. Generate predictions on test set\n",
    "5. Create `PREDICTIONS.CSV` for submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
